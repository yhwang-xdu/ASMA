This is the code to reproduce our algorithm.

## Reproduce Guide

### Base Project
This project is built upon the [DeepfakeBench](https://github.com/SCLBD/DeepfakeBench) repository. Please follow the instructions below to set up the environment, download the required datasets, and load the pre-trained weights.

### Datasets and Pretrained Detector Models
The datasets and detector models used in the experiments are downloaded from  [DeepfakeBench](https://github.com/SCLBD/DeepfakeBench), please put the dataset under `datasets` folder and pretrained weight under `training/pretrained` folder.

### Face Parsing Model

Our face parsing model is based on [DML_CSR](https://github.com/deepinsight/insightface/tree/master/parsing/dml_csr) repository. Please download the pretrained model and put it under `dml_csr` folder.

### Adversarial Attack Method

In our experiments, we have used some adversarial attack algorithms to compare with the proposed algorithm. 
| Method    |                                                                                                    Paper                                                                          |       Code       |
|   :---:         |                                                                                                     :----:                                                                           |        :---:        |
| FGSM        | Explaining and harnessing adversarial examples([Goodfellow et al., 2014])(https://arxiv.org/abs/1412.6572) | [Official Code](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html) |
|    PGD    | Towards Deep Learning Models Resistant to Adversarial Attacks([Mardry et al., 2017])(https://arxiv.org/abs/1706.06083) |      [Official Code](https://github.com/lts4/deepfool)       |
| C&W       | Towards Evaluating the Robustness of Neural Networks([Carlini et al., 2016])(https://arxiv.org/abs/1608.04644) | [Official Code](https://github.com/carlini/nn_robust_attacks) |
| Jitter | Exploring Misclassifications of Robust Neural Networks to Enhance Adversarial Attacks([Leo Schwinn et al., 2023])(https://doi.org/10.1007/s10489-023-04532-5) |  |
| BSR    | Boosting Adversarial Transferability by Block Shuffle and Rotation([Kunyu Wang et al., 2024])(https://arxiv.org/abs/2308.10299) | [Official Code](https://github.com/Trustworthy-AI-Group/BSR) |
| One Pixel | One Pixel Attack for Fooling Deep Neural Networks([Jiawei Su et al., 2019])(https://arxiv.org/abs/1710.08864) | [Official Code](https://github.com/Hyperparticle/one-pixel-attack-keras)                                                             |
### Experiment
#### Attack Success Rate Comparison
You can get the result by run the main.py:
```
$ python training/main.py [--parameters]
```
` --test_dataset` specifies the dataset you want to use for testing the attack. 

`--target_detector_path` is the path to the YAML file that contains the configuration for the target detector model. `--target_weights_path` is the path to the weights file for the target detector model, which contains the pre-trained weights.

`--attack_method` is used to specify the adversarial attack method that will be used for testing. You can choose methods from `fgsm, pgd, cw, df, ASMA, pASMA, jitter, or BSR`. The argument `--eps` specifies the epsilon value for the adversarial attack.

` --eval_detector_path` is the path to the YAML file for the evaluation detector configuration, while `--eval_weights_path` refers to the path to the weights file for the evaluation detector.

` --features` specifies the facial features that will be attacked when using the ASMA method. 

Here's an example:

```
$ python training/main.py     --target_detector_path ./training/config/detector/xception.yaml  --eval_detector_path ./training/config/detector/xception.yaml   --test_dataset "FaceForensics++"     --target_weights_path ./training/pretrained/xception_best.pth --eval_weights_path ./training/pretrained/xception_best.pth  --attack_method "ASMA"  --eps "2/255" 
```

#### Image Quality Assessment
You may want to observe the details of images quality generated by models, we add parameter `--iqa` to test the image quality:

```
$ python training/main.py [--parameters] --iqa 1
```
to output the QA metric values of the adversarial samples generated by the corresponding attack algorithm.
